{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import copy\n",
        "\n",
        "current_path = '/content/drive/MyDrive/Projects/NLP_HW1/'"
      ],
      "metadata": {
        "id": "kQymZ85QRH_g"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rgwF_99UyNr",
        "outputId": "e16ced9e-e803-4551-a551-ccf8d07a3582"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 2"
      ],
      "metadata": {
        "id": "We_tY1pCRMym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Read text"
      ],
      "metadata": {
        "id": "Icp6PJxRRY8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text(path):\n",
        "    \"\"\" Read text from the input path and return lines of it \"\"\"\n",
        "    f = open(path, 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    return lines\n",
        "\n",
        "train_lines = get_text(current_path + 'datasets/train.txt')"
      ],
      "metadata": {
        "id": "LWTR3gWnRLy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### create unigram LM and bigram LM"
      ],
      "metadata": {
        "id": "7BwJvSXVTogn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_LM(k_words, k_gram):\n",
        "    \"\"\" Update dictionary of LM \"\"\"\n",
        "    if k_words in k_gram.keys():\n",
        "        k_gram[k_words] += 1\n",
        "    else:\n",
        "        k_gram[k_words] = 1\n",
        "    return k_gram\n",
        "\n",
        "def create_LM(lines):\n",
        "    \"\"\" Create LM for unigram and bigram \"\"\"\n",
        "    unigram_LM = {}\n",
        "    bigram_LM = {}\n",
        "    for line in lines:\n",
        "        words = line.strip().split()\n",
        "        unigram_LM = update_LM(words[0], unigram_LM)\n",
        "        for i in range(1, len(words)):\n",
        "            biword = ' '.join(words[i-1: i+1])\n",
        "            uniword = words[i]\n",
        "            bigram_LM = update_LM(biword, bigram_LM)\n",
        "            unigram_LM = update_LM(uniword, unigram_LM)\n",
        "    return unigram_LM, bigram_LM\n",
        "\n",
        "def get_all_B(bigram_LM):\n",
        "    \"\"\" Calculate B variable in smoothing folrmula \"\"\"\n",
        "    all_B = {}\n",
        "    for biword in bigram_LM.keys():\n",
        "        word = biword.split()[0]\n",
        "        if word in all_B:\n",
        "            all_B[word] += 1\n",
        "        else:\n",
        "            all_B[word] = 1\n",
        "    return all_B"
      ],
      "metadata": {
        "id": "g-R6yYBcRdQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_LM, bigram_LM = create_LM(train_lines)\n",
        "words_size = sum(unigram_LM.values())\n",
        "vocab_size = len(unigram_LM.keys())\n",
        "all_B = get_all_B(bigram_LM)"
      ],
      "metadata": {
        "id": "55tfR8fMUhc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### absolute discount smoothsing"
      ],
      "metadata": {
        "id": "TotxCOKDUnh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_smoothing(unigram_LM, delta):\n",
        "    \"\"\" Calculate probability of all uniwords and smooth it \"\"\"\n",
        "    smoothed_unigram = copy.copy(unigram_LM)\n",
        "    for uniword in unigram_LM.keys():\n",
        "        alpha = (delta/words_size)*vocab_size\n",
        "        smoothed_unigram[uniword] = (max(unigram_LM[uniword] - delta, 0))/words_size + alpha*(1/vocab_size)\n",
        "    return smoothed_unigram\n",
        "        \n",
        "def bigram_smoothing(bigram_LM, unigram_LM, delta):\n",
        "    \"\"\" Calculate probability of all biiwords and smooth it \"\"\"\n",
        "    smoothed_bigram = copy.copy(bigram_LM)\n",
        "    global all_B\n",
        "    for biword in bigram_LM.keys():\n",
        "        words = biword.split()\n",
        "        B = all_B[words[0]]\n",
        "        alpha1 = (delta/unigram_LM[words[0]])*B\n",
        "        alpha2 = (delta/words_size)*vocab_size\n",
        "        smoothed_bigram[biword] = (max(bigram_LM[biword] - delta, 0))/unigram_LM[words[0]] + \\\n",
        "                                                                alpha1*((max(unigram_LM[words[1]] - delta, 0))/words_size + \\\n",
        "                                                                           alpha2*(1/vocab_size))\n",
        "    return smoothed_bigram\n",
        "\n",
        "def get_prob(delta, n_gram, unigram_LM, bigram_LM=None):\n",
        "    \"\"\" Calculate prbability of the input ngram \"\"\"\n",
        "    global all_B\n",
        "    if bigram_LM:\n",
        "        if n_gram in bigram_LM.keys():\n",
        "            return bigram_LM[n_gram]\n",
        "        else:\n",
        "            w2 = n_gram.split()[1]\n",
        "            if w2 in unigram_LM.keys():\n",
        "                return unigram_LM[w2]\n",
        "            else:\n",
        "                alpha = (delta/words_size)*vocab_size\n",
        "                return alpha*(1/vocab_size)                \n",
        "    else:\n",
        "        if n_gram in unigram_LM.keys():\n",
        "            return unigram_LM[n_gram]\n",
        "        else:\n",
        "            alpha = (delta/words_size)*vocab_size\n",
        "            return alpha*(1/vocab_size)"
      ],
      "metadata": {
        "id": "0krPT_qwUsj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Calculate perplexity"
      ],
      "metadata": {
        "id": "LxuXnE60Uo5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_perplexity(delta, n_gram, unigram_LM, bigram_LM=None):\n",
        "    \"\"\" Calculate perplexity of the input ngram. If bigram_LM is None just unigram is used in perplexity \"\"\"\n",
        "    prob = 0\n",
        "    for i in range(1, len(n_gram)):\n",
        "        if bigram_LM:\n",
        "            prob += math.log(get_prob(delta, ' '.join(n_gram[i-1:i+1]), unigram_LM, bigram_LM))\n",
        "        else:\n",
        "            prob += math.log(get_prob(delta, n_gram[i], unigram_LM))\n",
        "            \n",
        "    return prob * (-1/len(n_gram))\n",
        "\n",
        "def evaluate_LM(path, delta, unigram_LM, bigram_LM=None):\n",
        "    \"\"\" Evaluate LM by calculating perplexity \"\"\"\n",
        "    val_lines = get_text(path)    \n",
        "    all_perplexity = []\n",
        "    for line in val_lines:\n",
        "        if bigram_LM:\n",
        "            perplexity = get_perplexity(delta, line.split(), unigram_LM, bigram_LM)\n",
        "        else:\n",
        "            perplexity = get_perplexity(delta, line.split(), unigram_LM)\n",
        "        all_perplexity.append(math.e ** perplexity)\n",
        "    avg_perplexity = sum(all_perplexity)/len(all_perplexity)\n",
        "    return avg_perplexity\n"
      ],
      "metadata": {
        "id": "lJC3J2tnV3NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Find best delta"
      ],
      "metadata": {
        "id": "NECNHojrW6FA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_unigram_perplexity = math.inf\n",
        "best_unigram_delta = None\n",
        "min_bigram_perplexity = math.inf\n",
        "best_bigram_delta = None\n",
        "for i in range(5, 100, 5):   ## Change delta from 0 to 1 by 0.05 step\n",
        "    delta = round(0.01*i, 2)\n",
        "    smoothed_unigram_LM = unigram_smoothing(unigram_LM, delta)\n",
        "    smoothed_bigram_LM = bigram_smoothing(bigram_LM, unigram_LM, delta)\n",
        "    unigram_perplexity = evaluate_LM(current_path + 'datasets/valid.txt', delta, smoothed_unigram_LM)\n",
        "    bigram_perplexity = evaluate_LM(current_path + 'datasets/valid.txt', delta, smoothed_unigram_LM, smoothed_bigram_LM)\n",
        "    print('delta: ' + str(delta))\n",
        "    print('\\t unigram perplexity: ' + str(unigram_perplexity))\n",
        "    print('\\t bigram perplexity: ' + str(bigram_perplexity))\n",
        "    if min_unigram_perplexity > unigram_perplexity:\n",
        "        min_unigram_perplexity = unigram_perplexity\n",
        "        best_unigram_delta = delta\n",
        "    if min_bigram_perplexity > bigram_perplexity:\n",
        "        min_bigram_perplexity = bigram_perplexity\n",
        "        best_bigram_delta = delta\n",
        "\n",
        "print(\"#########################\")\n",
        "print('best results in validation:')\n",
        "print('unigram:')\n",
        "print('\\t best delta: ' + str(best_unigram_delta))\n",
        "print('\\t perplexity: ' + str(min_unigram_perplexity))\n",
        "print('bigram:')\n",
        "print('\\t best delta: ' + str(best_bigram_delta))\n",
        "print('\\t perplexity: ' + str(min_bigram_perplexity))\n",
        "\n",
        "smoothed_unigram_LM = unigram_smoothing(unigram_LM, best_unigram_delta)\n",
        "smoothed_bigram_LM = bigram_smoothing(bigram_LM, unigram_LM, best_bigram_delta)\n",
        "test_unigram_perplexity = evaluate_LM(current_path + 'datasets/test.txt', best_unigram_delta, smoothed_unigram_LM)\n",
        "test_bigram_perplexity = evaluate_LM(current_path + 'datasets/test.txt', best_unigram_delta, smoothed_unigram_LM, smoothed_bigram_LM)\n",
        "print(\"#########################\")\n",
        "print('best results in test:')\n",
        "print('unigram:')\n",
        "print('\\t best delta: ' + str(best_unigram_delta))\n",
        "print('\\t perplexity: ' + str(test_unigram_perplexity))\n",
        "print('bigram:')\n",
        "print('\\t best delta: ' + str(best_bigram_delta))\n",
        "print('\\t perplexity: ' + str(test_bigram_perplexity))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQOUSK80W_MN",
        "outputId": "be5e78f3-70ba-4eca-9e5e-42ef63af450a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "delta: 0.05\n",
            "\t unigram perplexity: 1360.7482165830145\n",
            "\t bigram perplexity: 896.5963299880291\n",
            "delta: 0.1\n",
            "\t unigram perplexity: 1255.3646534193872\n",
            "\t bigram perplexity: 812.9647001352185\n",
            "delta: 0.15\n",
            "\t unigram perplexity: 1204.0645156351343\n",
            "\t bigram perplexity: 773.4331753816832\n",
            "delta: 0.2\n",
            "\t unigram perplexity: 1171.4896572487612\n",
            "\t bigram perplexity: 749.1201087894766\n",
            "delta: 0.25\n",
            "\t unigram perplexity: 1148.1562951505282\n",
            "\t bigram perplexity: 732.3567453005434\n",
            "delta: 0.3\n",
            "\t unigram perplexity: 1130.2369272973428\n",
            "\t bigram perplexity: 720.0751862088389\n",
            "delta: 0.35\n",
            "\t unigram perplexity: 1115.8340926881492\n",
            "\t bigram perplexity: 710.7718366609752\n",
            "delta: 0.4\n",
            "\t unigram perplexity: 1103.8795086957416\n",
            "\t bigram perplexity: 703.6156941274498\n",
            "delta: 0.45\n",
            "\t unigram perplexity: 1093.7168573124022\n",
            "\t bigram perplexity: 698.1127853932402\n",
            "delta: 0.5\n",
            "\t unigram perplexity: 1084.9163131460652\n",
            "\t bigram perplexity: 693.9589784069473\n",
            "delta: 0.55\n",
            "\t unigram perplexity: 1077.182274391375\n",
            "\t bigram perplexity: 690.969174997163\n",
            "delta: 0.6\n",
            "\t unigram perplexity: 1070.3034307584817\n",
            "\t bigram perplexity: 689.0424890280086\n",
            "delta: 0.65\n",
            "\t unigram perplexity: 1064.1239114179427\n",
            "\t bigram perplexity: 688.1476969407297\n",
            "delta: 0.7\n",
            "\t unigram perplexity: 1058.525712435501\n",
            "\t bigram perplexity: 688.3240066509783\n",
            "delta: 0.75\n",
            "\t unigram perplexity: 1053.417520931823\n",
            "\t bigram perplexity: 689.6997506383678\n",
            "delta: 0.8\n",
            "\t unigram perplexity: 1048.727345333057\n",
            "\t bigram perplexity: 692.5431973426023\n",
            "delta: 0.85\n",
            "\t unigram perplexity: 1044.397503571306\n",
            "\t bigram perplexity: 697.3918206541689\n",
            "delta: 0.9\n",
            "\t unigram perplexity: 1040.3811232834391\n",
            "\t bigram perplexity: 705.4352635062517\n",
            "delta: 0.95\n",
            "\t unigram perplexity: 1036.6396408409998\n",
            "\t bigram perplexity: 720.1470084676779\n",
            "#########################\n",
            "best results in validation:\n",
            "unigram:\n",
            "\t best delta: 0.95\n",
            "\t perplexity: 1036.6396408409998\n",
            "bigram:\n",
            "\t best delta: 0.65\n",
            "\t perplexity: 688.1476969407297\n",
            "#########################\n",
            "best results in test:\n",
            "unigram:\n",
            "\t best delta: 0.95\n",
            "\t perplexity: 1040.813171510543\n",
            "bigram:\n",
            "\t best delta: 0.65\n",
            "\t perplexity: 677.4662636794408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3"
      ],
      "metadata": {
        "id": "d0TWgOPVXiqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test LM by predicting next word of incomplited sentences"
      ],
      "metadata": {
        "id": "_Jdnew0TXmo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_words(n_gram, n_next_words, unigram_LM, bigram_LM):\n",
        "    \"\"\" Predict next word of the input ngram using LM. If bigram_LM is None unigram LM is used else bigram LM \"\"\"\n",
        "    if bigram_LM:\n",
        "        for i in range(n_next_words):\n",
        "            condidates = [w for w in bigram_LM.keys() if w.split()[0]==n_gram[-1]]\n",
        "            best_condidate = max(condidates, key=lambda x:bigram_LM[x])\n",
        "            n_gram.append(best_condidate.split()[1])\n",
        "        return ' '.join(n_gram)\n",
        "    else:\n",
        "        for i in range(n_next_words):\n",
        "            best_condidate = max(unigram_LM, key=lambda x:unigram_LM[x])\n",
        "            n_gram.append(best_condidate)\n",
        "        return ' '.join(n_gram)       \n",
        "\n",
        "def complete_text_by_LM(unigram_LM, bigram_LM=None):\n",
        "    \"\"\" Test LM by predicting next word of all incomplete sentences of test file \"\"\"\n",
        "    f = open(current_path + 'datasets/test_incomplete.txt', 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    f = open(current_path + 'datasets/test_incomplete_gold.txt', 'r', encoding='UTF-8')\n",
        "    gold_lines = f.readlines()\n",
        "    f.close()\n",
        "    for i, line in enumerate(lines):\n",
        "        print('{0} test {1}:'.format('bigram' if bigram_LM else 'unigram', i+1))\n",
        "        print('incompleted: ' + line.split('###')[1].strip())\n",
        "        parts = line.strip().split('###')\n",
        "        n_incomplite = int(parts[0])\n",
        "        n_gram = parts[1].split()\n",
        "        complite_text = predict_next_words(n_gram, n_incomplite, unigram_LM, bigram_LM)\n",
        "        print('completed: ' + gold_lines[i].strip())\n",
        "        print('predicted: ' + complite_text+ '\\n')\n",
        "        \n",
        "complete_text_by_LM(smoothed_unigram_LM)  ### unigram LM test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yQimZiRXki_",
        "outputId": "7e51643b-6b62-4a51-83d9-072c35ee648b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram test 1:\n",
            "incompleted: این سخن حقست اگر نزد سخن گستر\n",
            "completed: این سخن حقست اگر نزد سخن گستر برند\n",
            "predicted: این سخن حقست اگر نزد سخن گستر و\n",
            "\n",
            "unigram test 2:\n",
            "incompleted: آنکه با یوسف صدیق چنین خواهد\n",
            "completed: آنکه با یوسف صدیق چنین خواهد کرد\n",
            "predicted: آنکه با یوسف صدیق چنین خواهد و\n",
            "\n",
            "unigram test 3:\n",
            "incompleted: هیچ دانی چکند صحبت او با\n",
            "completed: هیچ دانی چکند صحبت او با دگران\n",
            "predicted: هیچ دانی چکند صحبت او با و\n",
            "\n",
            "unigram test 4:\n",
            "incompleted: سرمه دهی بصر بری سخت خوش است\n",
            "completed: سرمه دهی بصر بری سخت خوش است تاجری\n",
            "predicted: سرمه دهی بصر بری سخت خوش است و\n",
            "\n",
            "unigram test 5:\n",
            "incompleted: آتش ابراهیم را\n",
            "completed: آتش ابراهیم را نبود زیان\n",
            "predicted: آتش ابراهیم را و و\n",
            "\n",
            "unigram test 6:\n",
            "incompleted: من که اندر سر\n",
            "completed: من که اندر سر جنونی داشتم\n",
            "predicted: من که اندر سر و و\n",
            "\n",
            "unigram test 7:\n",
            "incompleted: هر شیر شرزه را که به نیش\n",
            "completed: هر شیر شرزه را که به نیش سنان گزید\n",
            "predicted: هر شیر شرزه را که به نیش و و\n",
            "\n",
            "unigram test 8:\n",
            "incompleted: هرکه از حق به\n",
            "completed: هرکه از حق به سوی او نظریست\n",
            "predicted: هرکه از حق به و و و\n",
            "\n",
            "unigram test 9:\n",
            "incompleted: گفت این از\n",
            "completed: گفت این از خدای باید خواست\n",
            "predicted: گفت این از و و و\n",
            "\n",
            "unigram test 10:\n",
            "incompleted: کلاه لاله که لعل است\n",
            "completed: کلاه لاله که لعل است اگر تو بشناسی\n",
            "predicted: کلاه لاله که لعل است و و و\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_text_by_LM(smoothed_unigram_LM, smoothed_bigram_LM)  ### bigram LM test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeyQnEO7YzBC",
        "outputId": "a1da9533-ac74-4b9d-e660-419a8ace2e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram test 1:\n",
            "incompleted: این سخن حقست اگر نزد سخن گستر\n",
            "completed: این سخن حقست اگر نزد سخن گستر برند\n",
            "predicted: این سخن حقست اگر نزد سخن گستر و\n",
            "\n",
            "bigram test 2:\n",
            "incompleted: آنکه با یوسف صدیق چنین خواهد\n",
            "completed: آنکه با یوسف صدیق چنین خواهد کرد\n",
            "predicted: آنکه با یوسف صدیق چنین خواهد کرد\n",
            "\n",
            "bigram test 3:\n",
            "incompleted: هیچ دانی چکند صحبت او با\n",
            "completed: هیچ دانی چکند صحبت او با دگران\n",
            "predicted: هیچ دانی چکند صحبت او با تو\n",
            "\n",
            "bigram test 4:\n",
            "incompleted: سرمه دهی بصر بری سخت خوش است\n",
            "completed: سرمه دهی بصر بری سخت خوش است تاجری\n",
            "predicted: سرمه دهی بصر بری سخت خوش است و\n",
            "\n",
            "bigram test 5:\n",
            "incompleted: آتش ابراهیم را\n",
            "completed: آتش ابراهیم را نبود زیان\n",
            "predicted: آتش ابراهیم را به دست\n",
            "\n",
            "bigram test 6:\n",
            "incompleted: من که اندر سر\n",
            "completed: من که اندر سر جنونی داشتم\n",
            "predicted: من که اندر سر و از\n",
            "\n",
            "bigram test 7:\n",
            "incompleted: هر شیر شرزه را که به نیش\n",
            "completed: هر شیر شرزه را که به نیش سنان گزید\n",
            "predicted: هر شیر شرزه را که به نیش و از\n",
            "\n",
            "bigram test 8:\n",
            "incompleted: هرکه از حق به\n",
            "completed: هرکه از حق به سوی او نظریست\n",
            "predicted: هرکه از حق به دست و از\n",
            "\n",
            "bigram test 9:\n",
            "incompleted: گفت این از\n",
            "completed: گفت این از خدای باید خواست\n",
            "predicted: گفت این از آن را به\n",
            "\n",
            "bigram test 10:\n",
            "incompleted: کلاه لاله که لعل است\n",
            "completed: کلاه لاله که لعل است اگر تو بشناسی\n",
            "predicted: کلاه لاله که لعل است و از آن\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Part 4"
      ],
      "metadata": {
        "id": "m8rr4Dh3Xj3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Import liberaries"
      ],
      "metadata": {
        "id": "6KpD7VhIZH0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pZyvZHv97t5W"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import one_hot\n",
        "\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Extract bigrams and trigrams from text"
      ],
      "metadata": {
        "id": "kQ-EgMdJZO2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Q2NY71Pqzd4q"
      },
      "outputs": [],
      "source": [
        "def get_ngram(path, n):\n",
        "    \"\"\" Read text file of the input path and return all ngrams and next word of ngrams as labels \"\"\"\n",
        "    f = open(path, 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    n_grams = []\n",
        "    labels = []\n",
        "    # unique_ngrams = set()\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        for i in range(n, len(words)):\n",
        "            # if ' '.join(words[i-n:i+]) not in unique_ngrams:\n",
        "                # unique_ngrams.add(' '.join(words[i-n:i+1]))\n",
        "            n_grams.append(words[i-n:i])            \n",
        "            labels.append(words[i])\n",
        "    \n",
        "    return n_grams, labels\n",
        "\n",
        "bigrams, bigram_labels = get_ngram(current_path + 'datasets/train.txt', 2)\n",
        "bigrams_val, bigram_labels_val = get_ngram(current_path + 'datasets/valid.txt', 2)\n",
        "trigrams, trigram_labels = get_ngram(current_path + 'datasets/train.txt', 3)\n",
        "trigrams_val, trigram_labels_val = get_ngram(current_path + 'datasets/valid.txt', 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Encode ngrams to vectors"
      ],
      "metadata": {
        "id": "H_fyt-tic7Dy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iGVznKJGzuOP"
      },
      "outputs": [],
      "source": [
        "def get_vocabs(path):\n",
        "    \"\"\" Extract all vocabularies of tetx file of the input path \"\"\"\n",
        "    f = open(path, 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    vocabs = []\n",
        "    for line in lines:\n",
        "        vocabs.extend(line.split())\n",
        "    return list(set(vocabs))\n",
        "\n",
        "def encode_ngram(n_grams, word2id):\n",
        "    \"\"\" Encode ngrams of the input using word2id dictionary \"\"\"\n",
        "    encoded_ngrams = []\n",
        "    for n_gram in n_grams:\n",
        "        if type(n_gram) == str:\n",
        "            if n_gram in word2id.keys():\n",
        "                encoded_ngrams.append(word2id[n_gram])\n",
        "            else: \n",
        "                encoded_ngrams.append(word2id[''])\n",
        "        else:\n",
        "            encoded_ngram = []\n",
        "            for w in n_gram:\n",
        "                if w in word2id.keys():\n",
        "                    encoded_ngram.append(word2id[w])\n",
        "                else:\n",
        "                    encoded_ngram.append(word2id[''])\n",
        "            encoded_ngrams.append(encoded_ngram)\n",
        "    return np.array(encoded_ngrams)    \n",
        "        \n",
        "vocabs = get_vocabs(current_path + 'datasets/train.txt')\n",
        "word2id = {w:i+1 for i, w in enumerate(vocabs)}  ## assign a unique number to any words of vocabs\n",
        "word2id[''] = 0\n",
        "with open(current_path + 'word2id.pkl', 'wb') as f:\n",
        "    pickle.dump(word2id, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "encoded_bigrams = encode_ngram(bigrams, word2id)\n",
        "encoded_bigram_labels = encode_ngram(bigram_labels, word2id)\n",
        "encoded_bigrams_val = encode_ngram(bigrams_val, word2id)\n",
        "encoded_bigram_labels_val = encode_ngram(bigram_labels_val, word2id)\n",
        "\n",
        "encoded_trigrams = encode_ngram(trigrams, word2id)\n",
        "encoded_trigrams_labels = encode_ngram(trigram_labels, word2id)\n",
        "encoded_trigrams_val = encode_ngram(trigrams_val, word2id)\n",
        "encoded_trigrams_labels_val = encode_ngram(trigram_labels_val, word2id)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create keras model"
      ],
      "metadata": {
        "id": "JWBeeNCNeN8C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4V3jXegczyNB"
      },
      "outputs": [],
      "source": [
        "def get_model(word2id, n_gram):\n",
        "    vocab_size = len(word2id)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 64, input_length=len(n_gram[0])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "bigram_model = get_model(word2id, bigrams)\n",
        "trigram_model = get_model(word2id, trigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train bigram model then save it"
      ],
      "metadata": {
        "id": "jKqu2DCSeeDg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3BgotH9Wkw9",
        "outputId": "40439d51-277f-496c-8414-b8724c0c8937"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "744/744 [==============================] - 89s 116ms/step - loss: 7.7642 - accuracy: 0.0525 - val_loss: 7.3916 - val_accuracy: 0.0637\n",
            "Epoch 2/7\n",
            "744/744 [==============================] - 86s 116ms/step - loss: 7.1949 - accuracy: 0.0732 - val_loss: 7.2363 - val_accuracy: 0.0780\n",
            "Epoch 3/7\n",
            "744/744 [==============================] - 86s 115ms/step - loss: 6.9155 - accuracy: 0.0859 - val_loss: 7.1755 - val_accuracy: 0.0817\n",
            "Epoch 4/7\n",
            "744/744 [==============================] - 85s 115ms/step - loss: 6.6400 - accuracy: 0.0964 - val_loss: 7.1800 - val_accuracy: 0.0837\n",
            "Epoch 5/7\n",
            "744/744 [==============================] - 86s 115ms/step - loss: 6.3746 - accuracy: 0.1060 - val_loss: 7.2228 - val_accuracy: 0.0848\n",
            "Epoch 6/7\n",
            "744/744 [==============================] - 85s 115ms/step - loss: 6.1210 - accuracy: 0.1166 - val_loss: 7.2935 - val_accuracy: 0.0840\n",
            "Epoch 7/7\n",
            "744/744 [==============================] - 85s 115ms/step - loss: 5.8798 - accuracy: 0.1291 - val_loss: 7.3942 - val_accuracy: 0.0853\n"
          ]
        }
      ],
      "source": [
        "bigram_model.fit(encoded_bigrams, encoded_bigram_labels, batch_size=1024, epochs=7, shuffle=True, validation_data=(encoded_bigrams_val, encoded_bigram_labels_val))\n",
        "# save the model to file\n",
        "bigram_model.save(current_path + 'bigram_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train trigram model then save it"
      ],
      "metadata": {
        "id": "UPo26BdAeneo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD0yAQjhz1Uo",
        "outputId": "30728e20-28d6-46e9-9f4e-fa685d3a5401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "597/597 [==============================] - 69s 116ms/step - loss: 7.8734 - accuracy: 0.0510 - val_loss: 7.5043 - val_accuracy: 0.0554\n",
            "Epoch 2/7\n",
            "597/597 [==============================] - 69s 115ms/step - loss: 7.3038 - accuracy: 0.0655 - val_loss: 7.3550 - val_accuracy: 0.0720\n",
            "Epoch 3/7\n",
            "597/597 [==============================] - 69s 116ms/step - loss: 7.0184 - accuracy: 0.0801 - val_loss: 7.2778 - val_accuracy: 0.0786\n",
            "Epoch 4/7\n",
            "597/597 [==============================] - 68s 115ms/step - loss: 6.6963 - accuracy: 0.0918 - val_loss: 7.2672 - val_accuracy: 0.0808\n",
            "Epoch 5/7\n",
            "597/597 [==============================] - 69s 116ms/step - loss: 6.3646 - accuracy: 0.1030 - val_loss: 7.3173 - val_accuracy: 0.0816\n",
            "Epoch 6/7\n",
            "597/597 [==============================] - 69s 115ms/step - loss: 6.0397 - accuracy: 0.1160 - val_loss: 7.4192 - val_accuracy: 0.0816\n",
            "Epoch 7/7\n",
            "597/597 [==============================] - 68s 115ms/step - loss: 5.7278 - accuracy: 0.1336 - val_loss: 7.5368 - val_accuracy: 0.0813\n"
          ]
        }
      ],
      "source": [
        "trigram_model.fit(encoded_trigrams, encoded_trigrams_labels, batch_size=1024, epochs=7, validation_data=(encoded_trigrams_val, encoded_trigrams_labels_val))\n",
        "# save the model to file\n",
        "trigram_model.save(current_path + 'trigram_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load saved models "
      ],
      "metadata": {
        "id": "CkYJLNuFezTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.load_weights(current_path + 'bigram_model.h5')\n",
        "trigram_model.load_weights(current_path + 'trigram_model.h5')"
      ],
      "metadata": {
        "id": "xKbtlfDDDEDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "bigram_model = load_model(current_path + 'bigram_model.h5')\n",
        "trigram_model = load_model(current_path + 'trigram_model.h5')\n",
        "\n",
        "with open('word2id.pkl', 'rb') as f:\n",
        "    word2id = pickle.load(f)"
      ],
      "metadata": {
        "id": "zsodEBRyk_Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Calculate perplexity for neural LM model"
      ],
      "metadata": {
        "id": "854zXG9be-7I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cJApEPfFXo9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42fcbe9a-4efd-4fc1-8fd2-03bf89fdd252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity of neural bigram LM: 547.7307027241197\n",
            "perplexity of neural trigram LM: 473.6301467049531\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "def neural_perplexity(path, model, word2id, model_type):\n",
        "    id2word = {v:k for k,v in word2id.items()}\n",
        "    f = open(path, 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    all_perplexity = []\n",
        "    for line in lines:\n",
        "        words = line.split()\n",
        "        if model_type == 'bigram':\n",
        "            k = 2\n",
        "        else:\n",
        "            k = 3\n",
        "\n",
        "        ngrams = []\n",
        "        labels = []\n",
        "        for i in range(k, len(words)):\n",
        "            n_gram = encode_ngram(words[i-k:i], word2id)\n",
        "            label = encode_ngram([words[i]], word2id)[0]\n",
        "            ngrams.append(n_gram)\n",
        "            labels.append(label)\n",
        "        preds = model.predict(np.array(ngrams))\n",
        "        perplexity = 0\n",
        "        for i in range(len(preds)):\n",
        "            perplexity += math.log(preds[i][labels[i]])\n",
        "                \n",
        "        perplexity = perplexity * (-1/len(words[k:]))\n",
        "        all_perplexity.append(perplexity)\n",
        "    \n",
        "    avg_perplexity = sum(all_perplexity)/len(all_perplexity)\n",
        "    perplexity = math.e ** avg_perplexity\n",
        "    print('perplexity of neural {} LM: '.format(model_type) + str(perplexity))\n",
        "\n",
        "neural_perplexity(current_path + 'datasets/test.txt', bigram_model, word2id, 'bigram')\n",
        "neural_perplexity(current_path + 'datasets/test.txt', trigram_model, word2id, 'trigram')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Test LM by predcting next words of test data"
      ],
      "metadata": {
        "id": "DzNPpZtzhx4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_neural_LM(model, word2id, model_type):\n",
        "    id2word = {v:k for k,v in word2id.items()}\n",
        "    f = open(current_path + 'datasets/test_incomplete.txt', 'r', encoding='UTF-8')\n",
        "    lines = f.readlines()\n",
        "    f.close()\n",
        "    f = open(current_path + 'datasets/test_incomplete_gold.txt', 'r', encoding='UTF-8')\n",
        "    gold_lines = f.readlines()\n",
        "    f.close()\n",
        "    for i, line in enumerate(lines):\n",
        "        print('{0} test {1}:'.format(model_type, i+1))\n",
        "        print('incompleted: ' + line.split('###')[1].strip())\n",
        "        parts = line.strip().split('###')\n",
        "        n_incomplite = int(parts[0])\n",
        "        complite_text = parts[1]\n",
        "        for _ in range(n_incomplite):\n",
        "            if model_type == 'bigram':\n",
        "                n_gram = complite_text.split()[-2:]\n",
        "            else:\n",
        "                n_gram = complite_text.split()[-3:]\n",
        "            n_gram = encode_ngram(n_gram, word2id)\n",
        "            pred = model.predict(np.array([n_gram]))[0]\n",
        "            next_word = id2word[pred.argmax()]\n",
        "            complite_text = complite_text + ' ' + next_word\n",
        "        print('completed: ' + gold_lines[i].strip())\n",
        "        print('predicted: ' + complite_text + '\\n')\n",
        "\n",
        "test_neural_LM(bigram_model, word2id, 'bigram')"
      ],
      "metadata": {
        "id": "_e8P_lztHaQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_neural_LM(trigram_model, word2id, 'trigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4q5kk1cMh4A",
        "outputId": "4175e470-8f0f-4038-a17e-4ef4c892d810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram test 1:\n",
            "incompleted: این سخن حقست اگر نزد سخن گستر\n",
            "completed: این سخن حقست اگر نزد سخن گستر برند\n",
            "predicted: این سخن حقست اگر نزد سخن گستر بود\n",
            "\n",
            "trigram test 2:\n",
            "incompleted: آنکه با یوسف صدیق چنین خواهد\n",
            "completed: آنکه با یوسف صدیق چنین خواهد کرد\n",
            "predicted: آنکه با یوسف صدیق چنین خواهد کرد\n",
            "\n",
            "trigram test 3:\n",
            "incompleted: هیچ دانی چکند صحبت او با\n",
            "completed: هیچ دانی چکند صحبت او با دگران\n",
            "predicted: هیچ دانی چکند صحبت او با دل\n",
            "\n",
            "trigram test 4:\n",
            "incompleted: سرمه دهی بصر بری سخت خوش است\n",
            "completed: سرمه دهی بصر بری سخت خوش است تاجری\n",
            "predicted: سرمه دهی بصر بری سخت خوش است و\n",
            "\n",
            "trigram test 5:\n",
            "incompleted: آتش ابراهیم را\n",
            "completed: آتش ابراهیم را نبود زیان\n",
            "predicted: آتش ابراهیم را در جهان\n",
            "\n",
            "trigram test 6:\n",
            "incompleted: من که اندر سر\n",
            "completed: من که اندر سر جنونی داشتم\n",
            "predicted: من که اندر سر آن که\n",
            "\n",
            "trigram test 7:\n",
            "incompleted: هر شیر شرزه را که به نیش\n",
            "completed: هر شیر شرزه را که به نیش سنان گزید\n",
            "predicted: هر شیر شرزه را که به نیش می کند\n",
            "\n",
            "trigram test 8:\n",
            "incompleted: هرکه از حق به\n",
            "completed: هرکه از حق به سوی او نظریست\n",
            "predicted: هرکه از حق به دست حقورست و\n",
            "\n",
            "trigram test 9:\n",
            "incompleted: گفت این از\n",
            "completed: گفت این از خدای باید خواست\n",
            "predicted: گفت این از آن روی تو\n",
            "\n",
            "trigram test 10:\n",
            "incompleted: کلاه لاله که لعل است\n",
            "completed: کلاه لاله که لعل است اگر تو بشناسی\n",
            "predicted: کلاه لاله که لعل است و به هر\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}